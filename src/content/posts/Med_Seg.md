---
title: å‡ ç¯‡å…³äºåŒ»å­¦åˆ†å‰²çš„è®ºæ–‡è§£è¯»
published: 2025-09-30
description: 'è®ºæ–‡è§£è¯»'
image: ''
tags: [åŒ»å­¦å›¾åƒåˆ†å‰²]
category: 'æ·±åº¦å­¦ä¹ '
draft: false 
lang: 'zh_CN'
---

## ShaSpec- the first missing modality multi-modal approach 

### ä¸€äº›è¦çŸ¥é“çš„å†…å®¹

ä»€ä¹ˆæ˜¯æ¨¡æ€ï¼Ÿ

ä¿¡æ¯è¡¨è¾¾çš„å½¢å¼ï¼Œæ¯”å¦‚ç”¨æ–‡æœ¬æˆ–è€…è§†é¢‘å›¾ç‰‡ä¹‹ç±»çš„è¡¨è¾¾æŸä¸ªä¿¡æ¯



ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ï¼Ÿ

å¤šæ¨¡æ€æŒ‡çš„æ˜¯æ•°æ®æˆ–è€…ä¿¡æ¯çš„**å¤šç§**è¡¨ç°å½¢å¼





### Abstract

1. å½“å‰çš„æ–¹æ³•åœ¨evaluationæˆ–è€…train separate modelå»å¤„ç†ç‰¹å®šçš„æ¨¡æ€ç¼ºå¤±
2. these modelsï¼ˆæŒ‡çš„æ˜¯å“ªäº›ï¼Ÿï¼‰è‡´åŠ›äºå¤„ç†æŸä¸ªç‰¹å®šçš„ä»»åŠ¡(ä»ä¸‹æ–‡å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ–¹æ³•åœ¨åˆ†å‰²/ åˆ†ç±»ä¸Šé¢å¤„ç†çš„éƒ½æŒºä¸é”™çš„)

***Sha***red-***Spec***ific Feature Modelling å…±äº«ç‰¹å®šç‰¹å¾å»ºæ¨¡

#### å¦‚ä½•åšåˆ°ï¼Ÿ

1. ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data.  ShaSpecæ—¨åœ¨é€šè¿‡***å…±äº«å­¦ä¹ ***ï¼ˆå…±äº«å‚æ•°,å…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼‰å’Œ***å­¦ä¹ ç‰¹å®šç‰¹å¾***æ¥æ›´å¥½åœ°è¡¨ç¤ºè¾“å…¥æ•°æ®ï¼Œä»è€Œåœ¨è®­ç»ƒå’Œè¯„ä¼°æœŸé—´åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„è¾“å…¥æ¨¡æ€ã€‚
2. This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and domain classification, in addition to a residual feature fusion procedure. é€šè¿‡ä¾èµ–äºåŸºäºåˆ†å¸ƒå¯¹é½å’ŒåŸŸåˆ†ç±»çš„è¾…åŠ©ä»»åŠ¡ä»¥åŠå‰©ä½™ç‰¹å¾èåˆè¿‡ç¨‹çš„ç­–ç•¥æ¥å®ç°çš„ã€‚ï¼ˆè¿™é‡Œåé¢å¯ä»¥çœ‹åˆ°æ˜¯é€šè¿‡æ·»åŠ äº†ä¸¤ç§æ–°çš„æŸå¤±ç­–ç•¥æ¥è¾¾æˆçš„ï¼‰
3. The design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation. ShaSpecçš„è®¾è®¡ç®€å•æ€§ï¼ˆå¤§é“è‡³ç®€ï¼‰ä½¿å…¶æ˜“äºé€‚åº”å¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚åˆ†ç±»å’Œåˆ†æ®µ





![image-20250928233719430](imgs/med_seg/1-1.png)



![image-20250928231339150](imgs/med_seg/1-2.png)

#### note

è¿™ä¸¤å¼ å›¾çš„Decoderä»…ä»…ç”¨äºsegmentation

å¦‚æœè¦ç”¨äºclassificationï¼Œèåˆçš„ç‰¹å¾å°†è¢«å–‚ç»™FCå±‚ï¼Œ



#### ä½œè€…å¯¹è¿™äº›æ¨¡å—ä½œç”¨çš„è¯´æ˜

$$
s^{\{i\}}è¡¨ç¤ºçš„æ˜¯æ¨¡æ€é—´çš„å¼‚æ„æ€§ï¼Œr^{\{i\}}æ•æ‰ç‰¹å¾é—´çš„ä¸€è‡´æ€§
$$

#### ç¼ºå¤±æ¨¡æ€çš„è¯´æ˜

å…¶ä»–åœ°æ–¹æ˜¯ä¸€æ ·çš„ï¼Œåªæœ‰å¯¹ç¡®å®æ¨¡æ€ä¸­çš„fæ˜¯ç›´æ¥ç”Ÿæˆçš„


$$
å‡å®šnæ˜¯ç¼ºå¤±çš„æ¨¡æ€ï¼Œf^{(n)}=\frac{1}{N-1}\sum_{i=1,iâ‰ n}^Nr^{\{i\}}
$$
![image-20250929001546869](imgs/med_seg/1-3.png)





è¿™æ®µè¯æˆ‘æ²¡ææ‡‚ï¼Œå¦‚æœæœ‰â‰¥2çš„æ¨¡æ€ç¼ºå¤±çš„è¯ï¼Œé‚£åº”è¯¥å¦‚ä½•ç”Ÿæˆï¼Œå…¬å¼4ä¸åº”è¯¥åªç»™å‡ºäº†åªç¼ºå°‘å…¶ä¸­ä¸€ç§æ¨¡æ€çš„æƒ…å†µå—ï¼Ÿ

æ¨å¹¿åˆ°å¤šä¸ªæ¨¡æ€ç¼ºå¤±çš„å…¬å¼ä¸ºï¼š
$$
f^{(n)}=\frac{1}{å¯ç”¨æ¨¡æ€æ•°é‡}\sum_{i=1,iâ‰ n}^Nr^{\{i\}}
$$
å°±æ˜¯è¯´å…¶ä»–çš„ç¼ºå¤±æ¨¡æ€éƒ½ä¼šå˜æˆç›¸åŒçš„å€¼ï¼Œæ¯”å¦‚ç¼ºå¤±T1å’ŒT2,ç”¨FLå’ŒT1cçš„å¹³å‡å€¼æ¥è·å–ï¼Œæœ€ç»ˆT1å’ŒT2çš„å€¼ä¼šæ˜¯ç›¸åŒçš„











### è®­ç»ƒè¿‡ç¨‹

besides optimising for the main task (segmentation or classification), we introduce two auxiliary tasks, domain classification and distribution alignment, for the learning of the specific and shared feature representations, respectively. é™¤äº†å¯¹ä¸»è¦ä»»åŠ¡ï¼ˆåˆ†å‰²æˆ–åˆ†ç±»ï¼‰è¿›è¡Œä¼˜åŒ–ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼ŒåŸŸåˆ†ç±»å’Œåˆ†å¸ƒå¯¹é½ï¼Œåˆ†åˆ«ç”¨äºå­¦ä¹ ç‰¹å®šå’Œå…±äº«ç‰¹å¾è¡¨ç¤ºã€‚



æ–‡ä¸­å¹¶æ²¡æœ‰ä»‹ç»åˆ†å‰²å’Œåˆ†ç±»çš„è®­ç»ƒï¼Œè€Œæ˜¯ä»‹ç»äº†â€œDomain Classification Objectiveâ€å’Œâ€œ Distribution Alignment Objectiveâ€ä»¥åŠâ€œOverall Objectiveâ€



#### Domain Classification Objective

we propose to adopt the domain classification objective (DCO) for the specific feature learning. æå‡ºè¿™ä¸ªé˜ˆåˆ†ç±»ç›®æ ‡ç”¨äºç‰¹å®šç‰¹å¾å­¦ä¹ 


$$
L_{dco}(\mathcal{D},\theta^{spec},\theta^{dco})=-\sum_{j=1}^{|\mathcal{D}|}\sum_{i=1}^N(t^{(i)})^{\top}log(f_{\theta^{dco}}(s_j^{(i)}))\\
t^{(i)}\in\{0,1\}^N,å…¶ä¸­1æ˜¯ç¬¬iä¸ªä½ç½®ï¼Œå…¶ä»–éƒ½æ˜¯0ï¼Œæ¯”å¦‚è¯´å¯¹äºFlairæ¨¡æ€ï¼Œå®ƒçš„æ ‡ç­¾æ˜¯ [1, 0, 0, 0]ï¼›å¯¹äºT1æ¨¡æ€ï¼Œæ˜¯ [0, 1, 0, 0]ï¼Œä»¥æ­¤ç±»æ¨ã€‚\\
s^{(i)}=f_{\theta^{spec}}^{(i)}(x^{(i)})\\
$$


ç”¨äºè®¡ç®—ç‰¹å®šç¼–ç å™¨çš„æŸå¤±å€¼ï¼Œå…¶å®æ„Ÿè§‰å°±æ˜¯ç”¨äºä¼˜åŒ–Specific Encoder

Nï¼šæ¨¡æ€æ€»æ•°ï¼ˆ4ç§MRIæ¨¡æ€ï¼šFlair, T1, T1c, T2ï¼‰

Dï¼šè®­ç»ƒæ•°æ®é›†
$$
\mathcal{D}=\{(\mathcal{M}_j,y_j)\}_{j=1}^{|\mathcal{D}|}\\
\mathcal{M}_j:ç¬¬jä¸ªè®­ç»ƒæ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®é›†åˆï¼Œå…¶å®å°±ç±»ä¼¼äºä½ ç°æœ‰çš„æ•°æ®æ˜¯å¤šå°‘ï¼Œæ¢ä¸ªè¯´æ³•å°±æ˜¯x
$$






#### Distribution Alignment Objective

$$
L_{dao}(\mathcal{D},\theta^{sha},\theta^{dco})=-\sum_{j=1}^{|\mathcal{D}|}\sum_{i=1}^N(u^{(i)})^{\top}log(f_{\theta^{dao}}(r_j^{(i)}))\\
r^{(i)}=f_{\theta^{sha}}(x^{(i)})
$$



è¿™éƒ¨åˆ†å°±æ˜¯ç”¨äºä¼˜åŒ–Shared Encoderï¼Œ å¯ä»¥ä»å…¬å¼7å’Œ8çœ‹åˆ°ï¼Œè¿™éƒ¨åˆ†å…¶å®å°±æ˜¯è®¡ç®—

![image-20250929225446870](imgs/med_seg/1-10.png)

![image-20250929225455708](imgs/med_seg/1-11.png)





#### Overall Objective*ä¸»è¦


$$
L_{total}(\mathcal{D},\theta^{sha},\theta^{spec},\theta^{proj},\theta^{dao},\theta^{dco},\theta^{dec})=L_{task}(\mathcal{D},\theta^{sha},\theta^{spec},\theta^{proj},\theta^{dec})+\alpha L_{dao}(\mathcal{D},\theta^{sha},\theta^{dao})+\beta L_{dco}(\mathcal{D},\theta^{spec},\theta^{dco})
$$






#### æ•°æ®é›†ï¼š

a. **BraTS2018 for medical image segmentation** 

1.  The BraTS2018 Segmentation Challenge dataset [1,21] is used as a multi-modal learning with missing modality brain tumour sub-region segmentation benchmark,  ä»¥ä¸‹ä¸‰ä¸ªåˆ†å‰²å­åŒºåŸŸ
    1.  where the sub-regions are **enhancing tumour (ET)**
    2.  **tumour core (TC)**
    3.  **whole tumour (WT)**
2.  BraTS2018 contains 3D multi-modal brain MRIs, æ•°æ®é›†åŒ…å«ä»¥ä¸‹å››ç§æ¨¡æ€
    1.  including **Flair,** 
    2.  **T1,** 
    3.  **T1 contrast-enhanced (T1c)** 
    4.  **T2** with experienced imaging experts annotated ground-truth. 



| å‚æ•°                                                         | å…·ä½“å†…å®¹                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| stochastic gradient descent optimizer                        | Nesterov momentum of 0.99                                    |
| backbone network                                             | **3D UNet**(where the<br/>fusion of shared and specific features happens at the bottom<br/>of the UNet structure.) |
| learning rate                                                | 0.01 at the beginning and decreased with cosine annealing strategy  ä½™å¼¦é€€ç«ç­–ç•¥ |
| During the non-dedicated training of models, modalities are<br/>randomly dropped to simulate the modality-missing situations. | åœ¨æ¨¡å‹çš„éä¸“ç”¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡æ€è¢«éšæœºä¸¢å¼ƒä»¥æ¨¡æ‹Ÿæ¨¡æ€ç¼ºå¤±çš„æƒ…å†µã€‚ |
| For dedicated training of models, the missing modalities used for training are the same missing modalities in<br/>the evaluation. | å¯¹äºæ¨¡å‹çš„ä¸“ç”¨è®­ç»ƒï¼Œç”¨äºè®­ç»ƒçš„ç¼ºå¤±æ¨¡æ€ä¸evalä¸­çš„ç¼ºå¤±æ¨¡æ€ç›¸åŒã€‚ |
| iterations                                                   | 180,000                                                      |
| distribution alignment objective loss function               | L1 loss                                                      |
| Î± = 0.1, Î² = 0.02                                            |                                                              |
| ShaSpec*                                                     | prediction smoothness enhancement é¢„æµ‹å¹³æ»‘å¢å¼º               |

![image-20250929142014396](imgs/med_seg/1-4.png)





è¡¨ä¸€æ˜¯éä¸“ç”¨è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¸€ä¸ªæ¨¡å‹åº”å¯¹æ‰€æœ‰çš„æƒ…å†µï¼‰ï¼Œè¡¨2æ˜¯ä¸“ç”¨è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¤šä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹åº”å¯¹ä¸€ç§æƒ…å†µï¼‰

**è¿™ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**æ¨¡å‹æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯**è®­ç»ƒç­–ç•¥**ä¸åŒï¼Œ

**ä½†æ˜¯è¿™æ ·å­ä¸ºä»€ä¹ˆè¡¨ä¸€å’Œè¡¨äºŒåœ¨æ¨¡æ€ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå¯¹æŸä¸ªåŒºåŸŸè¿›è¡Œåˆ†å‰²æœ€ç»ˆè¾“å‡ºçš„ç»“æœä¸åŒï¼Ÿ**è§£é‡Šæ˜¯è¯´äºŒè€…æ‰€æä¾›çš„æ•°æ®å…¶å®æ˜¯ä¸åŒçš„ï¼ˆè®ºæ–‡æ˜¯è¿™æ ·å­è¯´æ˜çš„ï¼šDuring the non-dedicated training of models, modalities are randomly dropped to simulate the modality-missing situations. For dedicated training of models, the missing modalities used for training are the same missing modalities in the evaluation.åœ¨æ¨¡å‹çš„éä¸“ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡æ€è¢«éšæœºä¸¢å¼ƒä»¥æ¨¡æ‹Ÿæ¨¡æ€ç¼ºå¤±çš„æƒ…å†µã€‚å¯¹äºæ¨¡å‹çš„ä¸“ç”¨è®­ç»ƒï¼Œç”¨äºè®­ç»ƒçš„ç¼ºå¤±æ¨¡æ€ä¸è¯„ä¼°ä¸­çš„ç¼ºå¤±æ¨¡æ€ç›¸åŒã€‚ï¼‰<span style="color: red; font-weight: bold;">å°±æ˜¯è¯´éä¸“ç”¨æ¨¡å‹ç”¨æŸä¸ªæ¨¡æ€è®­ç»ƒï¼Œæœ€ç»ˆæµ‹è¯•çš„æ—¶å€™å¯ä»¥ç”¨ä»»æ„ç§æ¨¡æ€ä½œä¸ºæ•°æ®è¾“å…¥ï¼Œè€Œä¸“ç”¨è®­ç»ƒå°±æ˜¯è®­ç»ƒå’Œæµ‹è¯•çš„æ—¶å€™æ•°æ®éƒ½æ˜¯ç›¸åŒçš„</span>







b. **Audiovision-MNIST for computer vision classification**

1. éŸ³é¢‘æ‰‹å†™æ•°å­—é›†
2. a multi-modal dataset consisting of 1500 samples of audio and image files.
3. é‡‡ç”¨å’ŒSMILæ¨¡å‹ä¸€è‡´çš„å‚æ•°
4. ä¸åŒçš„æ˜¯æœ€åçš„Decoderæ˜¯ç”±FC-dropout-FCç»„æˆ
5. æ¨¡å‹è®­ç»ƒä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæƒé‡è¡°å‡ä¸º10âˆ’2ï¼Œåˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º10âˆ’3ï¼ˆæ¯20ä¸ªepochå‡å°‘10%ï¼‰



![image-20250929145301715](imgs/med_seg/1-5.png)

![image-20250929145701180](imgs/med_seg/1-6.png)



#### åšäº†ä¸€äº›å…¶ä»–çš„å®éªŒ

##### **The selection of DAO loss function**

L1ï¼KLï¼MSEï¼CE

![image-20250929145848163](imgs/med_seg/1-7.png)

##### **Sensitivity of Eq. (9) to Î± and Î²:** 

æµ‹è¯•Î± æ—¶å€™ï¼Œ Î²=0.02

æµ‹è¯• Î²æ—¶å€™ï¼ŒÎ± =0.1

Î± å’Œ Î²éƒ½ä¸º1çš„æ—¶å€™ï¼Œæ›²çº¿ä¸‹é™çš„å¾ˆå¿«ï¼Œè¡¨æ˜è¾…åŠ©æŸå¤±ç»™å¤ªé«˜æƒé‡ä¼šå¯¼è‡´main taskæ¢¯åº¦æµå—åˆ°å¹²æ‰°

![image-20250929145939768](imgs/med_seg/1-8.png)

Î± =0.1ï¼ŒÎ²=0.02æ˜¯æœ€ä¼˜è§£



**Small values for the weights of the auxiliary tasks contribute to the whole process, but do not interfere with the main task optimisation.** Interestingly, when Î± = 0 (only specific features are learned), the model can still segment the tumours to some extent by simple concatenation of specific features, which means that the specific features contain rich information. A similar conclusion can be reached when Î² = 0 (only shared features are learned). è¿™è¾¹è¯´è¾…åŠ©åˆ†æ”¯çš„æƒå€¼ç»™çš„å°å¯¹æ•´ä¸ªè®­ç»ƒæœ‰æå‡ï¼Œä½†å¹¶ä¸ä¼šå½±å“ä¸»ä»»åŠ¡çš„ä¼˜åŒ–ï¼Œå½“Î± or Î²=0æ—¶å€™ï¼Œæ¨¡å‹ä»ç„¶å¯ä»¥åˆ†å‰²ç›®æ ‡çš„ç‰¹å®šç‰¹å¾ï¼ˆå…¶å®æˆ‘æ„Ÿè§‰è¦æ˜¯è¾…åŠ©åˆ†æ”¯å¯¹æ•´ä½“çš„åˆ†å‰²éƒ½æœ‰å¤§çš„å½±å“ï¼Œé‚£å¯èƒ½éƒ½ä¸æ˜¯è¾…åŠ©äº†è€Œæ˜¯å¦ä¸€ä¸ªå’Œä¸»æ¨¡å‹åŒç­‰çš„æ¨¡å‹äº†ğŸ˜‚ï¼Œè¿™æ®µè¯å¤šå°‘æ˜¯æœ‰ç‚¹å‡‘å­—æ•°çš„æ„Ÿè§‰äº†ï¼‰



##### Computational comparison

we estimate the average time consumption of **30 iterations on one 3090 GPU** for a fair comparison.

|                                   | ShaSpec                                                      | SMIL                                                         |
| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| training/inference iteration time | takes **0.0257s** for model training iteration and **0.0016s** for model testing | training iterations and testing taking 0.1309s and 0.0019s   |
| GPU memory usage                  | constantly consumes **1421MiB** of GPU memory                | the GPU memory usage started<br/>from 1430MiB, climbed to 24268MiB, and then casted an<br/>â€œout of memoryâ€ error in the end. |
| batch-size                        | 4                                                            | 4                                                            |
| model parameters                  | **0.22M parameters**                                         | 0.33M parameters                                             |



##### An additional classification experiment on X-ray + clinical texts

ï¼ˆä¸ªäººè§‰å¾—å¯ä»¥passè¿™éƒ¨åˆ†ï¼‰å…¶å®å°±æ˜¯åœ¨ä¸€ä¸ªä¸´åºŠæ•°æ®é›†åˆ†ç±»ï¼ˆè§†è§‰-æ–‡æœ¬æ•°æ®é›†ï¼‰ä¸Šé¢åˆåšäº†ä¸€æ¬¡å®éªŒ



##### Shared and Specific Feature Visualisation

![image-20250929151534467](imgs/med_seg/1-9.png)

å…±äº«ç‰¹å¾è¢«èšç±»åœ¨ä¸€èµ·ï¼Œè€Œç‰¹å®šç‰¹å¾è¢«å¾ˆå¥½åœ°åˆ†ç¦»

å…¶å®æˆ‘æ¯”è¾ƒå¥½å¥‡è¿™éƒ¨åˆ†ä½œå›¾ä»£ç æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼ˆæ˜¾ç„¶çœ‹äº†ä»£ç è¿™éƒ¨åˆ†ä¼¼ä¹å¹¶æ²¡æœ‰åœ¨å†…ï¼‰





























